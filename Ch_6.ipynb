{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32008321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sympy\n",
    "import pandas\n",
    "import torch\n",
    "# import jtplot module in notebook\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e807259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(2 x + 2\\right) \\log{\\left(x \\right)} + \\frac{x^{2} + 2 x}{x}$"
      ],
      "text/plain": [
       "(2*x + 2)*log(x) + (x**2 + 2*x)/x"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미분법 예제\n",
    "# 0. 직접 미분\n",
    "\n",
    "x = sympy.Symbol('x')\n",
    "f = (x**2 + 2*x)*sympy.log(x)\n",
    "df = sympy.diff(f, x)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684c3c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x + 2 \\left(x + 1\\right) \\log{\\left(x \\right)} + 2$"
      ],
      "text/plain": [
       "x + 2*(x + 1)*log(x) + 2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.simplify(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e217bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 수치 미분\n",
    "\n",
    "def numer_deriv(f, x, h=0.001, method=\"center\"):\n",
    "    \"\"\"\n",
    "    f : 미분합 함수로 주어진 위치에서 함숫값 계산을 위해 사용\n",
    "    x : 미분계수를 구할 변수의 위치로,\n",
    "        일변수인 경우 Int, float\n",
    "        다변수인 경우 numpy array, vector\n",
    "    h : 비율을 구할 작은 구간\n",
    "    \"\"\"\n",
    "    if type(x) in (float, int):\n",
    "        grad = [0,0]\n",
    "        x_ = [x]\n",
    "        var_type = 'scalar'\n",
    "    else :\n",
    "        grad = np.zeros(x.shape)\n",
    "        x_ = x.copy().astype('float32')\n",
    "        var_type = 'vector'\n",
    "    \n",
    "    for i, xi in enumerate(x_):\n",
    "        original_value = x_[i]\n",
    "        \n",
    "        if method=='forward':\n",
    "            x_[i] = original_value + h\n",
    "        else:\n",
    "            x_[i] = original_value + (h/2)\n",
    "        \n",
    "        if var_type == 'scalar':\n",
    "            gradplus = f(x_[i])\n",
    "        else :\n",
    "            gradplus = f(x_)\n",
    "        \n",
    "        if method=='forward':\n",
    "            x_[i] = original_value\n",
    "        else :\n",
    "            x_[i] = original_value - (h/2)\n",
    "        \n",
    "        if var_type =='scalar':\n",
    "            gradminus = f(x_[i])\n",
    "        else :\n",
    "            gradminus = f(x_)\n",
    "        \n",
    "        grad[i] = (gradplus - gradminus) / h\n",
    "        \n",
    "    if var_type == 'scalar':\n",
    "        return grad[0]\n",
    "    else :\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574dfc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999999999999666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x : (x**2 + 2*x)*np.log(x)\n",
    "df = lambda x : 2*(x+1)*np.log(x) + (x+2)\n",
    "\n",
    "numer_deriv(f,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82815740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.257383635135726\n"
     ]
    }
   ],
   "source": [
    "print(numer_deriv(f,1,h=0.5, method=\"forward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f091b5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9997299032915508\n"
     ]
    }
   ],
   "source": [
    "print(numer_deriv(f,1,h=0.5, method=\"center\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f6c3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.77255299, 1.49889143])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_xy = lambda x : (x[0]**2 + 2*x[0])*np.log(x[1])\n",
    "numer_deriv(f_xy, np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506e5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2*x + 2)*log(y)\n",
      "(x**2 + 2*x)/y\n",
      "2.77258872223978\n",
      "1.50000000000000\n"
     ]
    }
   ],
   "source": [
    "x = sympy.Symbol('x')\n",
    "y = sympy.Symbol('y')\n",
    "f_xy_sympy = (x**2 + x*2)*sympy.log(y)\n",
    "df_xy_x = sympy.diff(f_xy_sympy, x)\n",
    "df_xy_y = sympy.diff(f_xy_sympy, y)\n",
    "\n",
    "print(df_xy_x)\n",
    "print(df_xy_y)\n",
    "print(f\"{df_xy_x.evalf(subs={x:1.0, y:2.0})}\")\n",
    "print(f\"{df_xy_y.evalf(subs={x:1.0, y:2.0})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b4cb084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(2 x + 2\\right) \\log{\\left(y \\right)}$"
      ],
      "text/plain": [
       "(2*x + 2)*log(y)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xy_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70dbfc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]] float64\n",
      "tensor([[0.5488, 0.7152, 0.6028],\n",
      "        [0.5449, 0.4237, 0.6459]], dtype=torch.float64) torch.float64 False\n",
      "tensor([[0.5488, 0.7152, 0.6028],\n",
      "        [0.5449, 0.4237, 0.6459]], dtype=torch.float64) torch.float64 False\n",
      "tensor([[0.5488, 0.7152, 0.6028],\n",
      "        [0.5449, 0.4237, 0.6459]]) torch.float32 False\n",
      "tensor([[0.5488, 0.7152, 0.6028],\n",
      "        [0.5449, 0.4237, 0.6459]], dtype=torch.float64) torch.float64 False\n"
     ]
    }
   ],
   "source": [
    "# 자동미분\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x = np.random.rand(6).reshape(2,3)\n",
    "\n",
    "x_tensor = torch.tensor(x)\n",
    "x_from_numpy = torch.from_numpy(x)\n",
    "x_Tensor = torch.Tensor(x)\n",
    "x_as_tensor = torch.as_tensor(x)\n",
    "\n",
    "print(x, x.dtype)\n",
    "print(x_tensor, x_tensor.dtype, x_tensor.requires_grad)\n",
    "print(x_from_numpy, x_from_numpy.dtype, x_from_numpy.requires_grad)\n",
    "print(x_Tensor, x_Tensor.dtype, x_Tensor.requires_grad) # float 32bits \n",
    "print(x_as_tensor, x_as_tensor.dtype, x_as_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59253cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.           0.71518937   0.60276338]\n",
      " [  0.54488318   0.4236548    0.64589411]] float64\n"
     ]
    }
   ],
   "source": [
    "x[0,0] = 100\n",
    "print(x,x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb5f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5488, 0.7152, 0.6028],\n",
      "        [0.5449, 0.4237, 0.6459]], dtype=torch.float64) torch.float64 False\n",
      "tensor([[100.0000,   0.7152,   0.6028],\n",
      "        [  0.5449,   0.4237,   0.6459]], dtype=torch.float64) torch.float64 False\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor,x_tensor.dtype, x_tensor.requires_grad)\n",
    "\n",
    "print(x_from_numpy, x_from_numpy.dtype, x_from_numpy.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e766559d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100.0000,   0.7152,   0.6028],\n",
      "        [  0.5449,   0.4237,   0.6459]], dtype=torch.float64,\n",
      "       requires_grad=True) torch.float64 True\n"
     ]
    }
   ],
   "source": [
    "x_tensor_grad = torch.tensor(x, requires_grad=True)\n",
    "print(x_tensor_grad, x_tensor_grad.dtype, x_tensor_grad.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d528d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([0.], grad_fn=<MulBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "f = (x**2 + 2*x) * torch.log(x)\n",
    "print(x)\n",
    "print(f)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34fbb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.backward(f, retain_graph=True) # memory에 남겨두는 역할\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094b9289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3.]),)\n"
     ]
    }
   ],
   "source": [
    "df = torch.autograd.grad(f, x, retain_graph=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e4595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([3.]), tensor([3.]))\n"
     ]
    }
   ],
   "source": [
    "df = torch.autograd.grad(f,(x,x), retain_graph=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7f9fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7726])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "f_xy = (x**2 + 2*x) * torch.log(y)\n",
    "\n",
    "torch.autograd.backward(f_xy, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac96d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2.7726]), tensor([1.5000]))\n"
     ]
    }
   ],
   "source": [
    "df = torch.autograd.grad(f_xy, (x,y), retain_graph=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac711a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx=3, dy=2\n"
     ]
    }
   ],
   "source": [
    "# 자동미분 구현\n",
    "\n",
    "def times(x,y):\n",
    "    return x*y, (x, y)\n",
    "\n",
    "def times_deriv(cache, dout=1):\n",
    "    return cache[1]*dout, cache[0]*dout\n",
    "\n",
    "TIMES = {\n",
    "    'f' : times,\n",
    "    'df' : times_deriv\n",
    "}\n",
    "\n",
    "v, cache = TIMES['f'](2,3) # 함수 표현 방식 익혀두기.\n",
    "dx, dy = TIMES['df'](cache)\n",
    "\n",
    "print(\"dx={}, dy={}\".format(dx,dy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0b323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x, y):\n",
    "    return x+y, (x,y)\n",
    "\n",
    "def add_deriv(cache, dout=1):\n",
    "    return dout, dout\n",
    "\n",
    "ADD = {\n",
    "    'f' : add,\n",
    "    'df' : add_deriv\n",
    "}\n",
    "\n",
    "def log(x):\n",
    "    return np.log(x), x\n",
    "\n",
    "def log_deriv(cache, dout=1):\n",
    "    return (1/cache)*dout\n",
    "\n",
    "LOG = {\n",
    "    'f' : log,\n",
    "    'df' : log_deriv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4550b52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward pass dx = 2.772588722239781, dy = 1.5\n"
     ]
    }
   ],
   "source": [
    "x = 1.; y = 2.\n",
    "\n",
    "a, cache_a = TIMES['f'](x,x)\n",
    "b, cache_b = TIMES['f'](2,x)\n",
    "c, cache_c = ADD['f'](a,b)\n",
    "d, cache_d = LOG['f'](y)\n",
    "z, cache_z = TIMES['f'](c, d)\n",
    "\n",
    "dx = dy = 0.\n",
    "\n",
    "dc, dd = TIMES['df'](cache_z, 1)\n",
    "dy  = LOG['df'](cache_d, dd)\n",
    "da, db = ADD['df'](cache_c, dc)\n",
    "_, dx_ = TIMES['df'](cache_b, db); dx += dx_;\n",
    "dx_, dx__ = TIMES['df'](cache_a, da); dx += dx_ + dx__;\n",
    "\n",
    "print(f'backward pass dx = {dx}, dy = {dy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2940cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([5.5452]), tensor([3.]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = torch.tensor([2.], requires_grad=True)\n",
    "z = (x**2 + x*2)*torch.log(y)\n",
    "\n",
    "# grad_outputs : upstream derivative\n",
    "dz = torch.autograd.grad(z, (x,y), grad_outputs=torch.tensor([2.]), retain_graph=True)\n",
    "print(dz) # upstream derivative가 2이기 때문에 기존 derivative의 2배가 결과로 나왔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d49594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
